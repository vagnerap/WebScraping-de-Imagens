{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import shutil\n",
    "from urllib.request import urlopen\n",
    "import numpy\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#essa é a função utilizada para fazer download das imagens\n",
    "\n",
    "def download_image(image):\n",
    "    response = requests.get(image['src'], stream=True)\n",
    "    realname = ''.join(e for e in image['alt'] if e.isalnum())\n",
    "    \n",
    "    file = open(\"C:/Users/vagne/Desktop/achei_fotos/{}.png\".format(realname), 'wb')\n",
    "    \n",
    "    response.raw.decode_content = True\n",
    "    shutil.copyfileobj(response.raw, file)\n",
    "    del response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a partir daqui é o processo de scraping da página\n",
    "\n",
    "#Eu abri o site com o urllib, depois peguei o html dele com o beautifulSoup, com a url soup_list\n",
    "url_list = urlopen('https://querobolsa.com.br/faculdades-e-universidades/todas')\n",
    "soup_list = BeautifulSoup(url_list, 'html.parser')\n",
    "\n",
    "#A página é uma lista com milhares de links, aqui no get_htmls_of_url_list eu to setando o link de cada página\n",
    "get_htmls_of_url_list = soup_list.find_all('a','university-directory__content-list-item')\n",
    "\n",
    "#all_images é onde eu irei armazenar os links das imagens\n",
    "all_images = []\n",
    "\n",
    "#nesse for eu to definindo que ele pegue o a de todas as páginas, acesse ele, e faça o mesmo processo de abrir a págine\n",
    "#e pegue o html dela\n",
    "for aas in get_htmls_of_url_list:\n",
    "    url_completa = 'https://querobolsa.com.br'+aas.get('href')\n",
    "    url_comp = urlopen(url_completa)       \n",
    "    \n",
    "    html_inside_ies = BeautifulSoup(url_comp, 'html.parser')    \n",
    "    images = html_inside_ies.find_all('img', {'src':re.compile('.png')})             \n",
    "    \n",
    "    #nesse for é onde a mágia acontece, pois eu pego cada página, puxo a imagem e o nome dela e baixo\n",
    "    #o time.sleep é para que não sejam feitas todas as requisições de uma vez, evitando time out na página, então a cada 5 segundos ele executa o for\n",
    "    for image in images:         \n",
    "        dict_with_src_alt = {'src': image['src'], 'alt': image['alt']}\n",
    "        download_all = download_image(dict_with_src_alt)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
